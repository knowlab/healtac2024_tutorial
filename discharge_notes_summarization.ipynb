{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HealTac 2024 Tutorial\n",
        "## Instruction Tuning for Discharge Notes Summarization\n",
        "\n",
        "- Yunsoo Kim (yunsoo.kim.23@ucl.ac.uk), Jinge Wu (jinge.wu.20@ucl.ac.uk), Honghan Wu (honghan.wu@ucl.ac.uk)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/knowlab/healtac_2024_tutorial/blob/main/discharge_notes_summarization.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "PgS8fQu0dju7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the runtime to be T4 GPU.  \n",
        "\n",
        "We will get started with installing packages and downloading the model because they take some time."
      ],
      "metadata": {
        "id": "vcXEvrKFeVG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run nvidia-smi to check the gpu resource\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Aoe8KWfRfKQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLdwofkkdfEt"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "!pip install -q accelerate==0.25.0 peft==0.6.2 bitsandbytes==0.41.1 transformers==4.36.2 trl==0.7.4 einops gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import torch\n",
        "from datasets import load_dataset # loading the dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig # for LoRA\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM # Trainer and DataCollator\n",
        "import gradio as gr # for deployment"
      ],
      "metadata": {
        "id": "7D953H5Xedjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Quantization Config\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "# Load Model\n",
        "# We will use Phi-2\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    revision=\"refs/pr/23\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_sight = \"right\"\n",
        "\n",
        "# Load Dataset\n",
        "dataset = load_dataset(\"bluesky333/synthetic_discharge_summ\")"
      ],
      "metadata": {
        "id": "g4dbN3Bzefu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFT Markup Description\n",
        "\n",
        "LoRA\n",
        "\n",
        "Quantization"
      ],
      "metadata": {
        "id": "VYRWZT__j7RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's have a look at the train dataset\n",
        "print(dataset['train'])\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "OMoZKdyTe4Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the test dataset\n",
        "print(dataset['test'])\n",
        "dataset['test'][0]"
      ],
      "metadata": {
        "id": "8EI5xgRN2LF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We make this dataset to phi-2 compatible\n",
        "# Phi-2 instruction-answer format: \"Instruct: <prompt>\\nOutput:\"\n",
        "\n",
        "# Make your own prompt!\n",
        "prompt_template=\"\"\"Instruct: Please write down your own prompt.\n",
        "For instance, you can insert the note as {{note}}\n",
        "{note}\n",
        "Model should answer to {{question}} based on the note.\n",
        "{question}\n",
        "You should maintain the phi-2 format\n",
        "Accordingly, the last line must be like the below.\n",
        "Do not forget to insert a new line between your prompt and 'Output'!\n",
        "Output: {answer}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template=\"\"\"Instruct: Answer the question about the following clinical note. \\n{note}.\n",
        "Output: {answer}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Should get Dict[List] as input, return list of prompts\n",
        "def format_dataset(samples):\n",
        "    outputs = []\n",
        "    for _, note, question, answer in zip(*samples.values()):\n",
        "        out = prompt_template.format(note=note, question=question, answer=answer)\n",
        "        outputs.append(out)\n",
        "    return outputs\n",
        "\n",
        "sample_input = format_dataset({k: [v] for k, v in dataset['train'][0].items()})[0]\n",
        "print(sample_input)\n",
        "print(\"*\"*20)\n",
        "\n",
        "# Sanity Check\n",
        "prompt_len = len(tokenizer.encode(prompt_template))\n",
        "if prompt_len > 180:\n",
        "    raise ValueError(f\"Your prompt is too long! Please reduce the length from {prompt_len} to 180 tokens\")\n",
        "print(f\"Prompt Length: {prompt_len} tokens\")"
      ],
      "metadata": {
        "id": "ujW2fAlue6NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "sample_input = format_dataset({k: [v] for k, v in dataset['train'][sample_idx].items()})[0].split('Output: ')[0]\n",
        "input_ids = tokenizer.encode(sample_input, return_tensors='pt').to('cuda')\n",
        "with torch.no_grad():\n",
        "  output = model.generate(input_ids=input_ids,\n",
        "                            max_length=512,\n",
        "                            use_cache=True,\n",
        "                            temperature=0.,\n",
        "                            eos_token_id=tokenizer.eos_token_id\n",
        "  )\n",
        "print(tokenizer.decode(output.to('cpu')[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "70z9VMYUfUyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, let's define dataset.\n",
        "response_template = \"Output:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "train_dataset = dataset['train']"
      ],
      "metadata": {
        "id": "ApZOSbzafWP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFTTrainer Do everything else for you!\n",
        "\n",
        "lora_config=LoraConfig(\n",
        "    r=4,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules= [\"Wqkv\", \"fc1\", \"fc2\" ]\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_strategy=\"no\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=None,\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    formatting_func=format_dataset,\n",
        "    data_collator=collator,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "lGJAv8hLfXuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "pPceawKOfZcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap-up Training\n",
        "model = trainer.model\n",
        "model.eval()\n",
        "\n",
        "note_samples = dataset['test']['note']\n",
        "\n",
        "def inference(note, question, model):\n",
        "    prompt = prompt_template.format(note=note, question=question, answer=\"\")\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
        "    outs = model.generate(input_ids=tokens,\n",
        "                          max_length=512,\n",
        "                          use_cache=True,\n",
        "                          temperature=1.,\n",
        "                          eos_token_id=tokenizer.eos_token_id\n",
        "                          )\n",
        "    output_text = tokenizer.decode(outs.to('cpu')[0], skip_special_tokens=True)\n",
        "    return output_text[len(prompt):]\n",
        "\n",
        "\n",
        "def compare_models(note, question):\n",
        "    with torch.no_grad():\n",
        "        asc_answer = inference(note, question, trainer.model)\n",
        "        with model.disable_adapter():\n",
        "            phi_answer = inference(note, question, trainer.model)\n",
        "    return asc_answer, phi_answer\n",
        "\n",
        "demo = gr.Interface(fn=compare_models, inputs=[gr.Dropdown(note_samples), \"text\"], outputs=[gr.Textbox(label=\"Trained\"), gr.Textbox(label=\"Phi-2\")])\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "Nz_Z_Gj1favb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ogayHIqhHKrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}